{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuGTJZyOs2ec"
      },
      "source": [
        "# **1.Import Pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o9p9VvQZspbf"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchsummary'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m     12\u001b[0m no_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m use_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m no_cuda \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models.feature_extraction as feature_extraction\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "from torchsummary import summary\n",
        "\n",
        "no_cuda = False\n",
        "use_gpu = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhWHQDwCs75O"
      },
      "source": [
        "# **2.Load Fashion MNIST Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl04W5rGs7Pc",
        "outputId": "d2d6603f-caa7-434b-810b-ea5a904b0bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:19<00:00, 1384050.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 94440.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4422102/4422102 [00:10<00:00, 436859.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6613254.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#Dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "#Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxP37BOvunNX"
      },
      "source": [
        "# **3.Create a NN model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7TjEZwbuljD"
      },
      "outputs": [],
      "source": [
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.nn1 = nn.Linear(28*28, 120)\n",
        "    self.nn2 = nn.Linear(120, 84)\n",
        "    self.nn3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    x = F.relu(self.nn1(x))\n",
        "    x = F.relu(self.nn2(x))\n",
        "    x = self.nn3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkPcH4p9w3d8",
        "outputId": "3b26c0e6-76d6-42e9-c11b-15756a8d20e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 120]          94,200\n",
            "            Linear-2                   [-1, 84]          10,164\n",
            "            Linear-3                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 105,214\n",
            "Trainable params: 105,214\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.40\n",
            "Estimated Total Size (MB): 0.41\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#Print summary of model\n",
        "FP32_model = ToyModel().to(device)\n",
        "summary(FP32_model,(1,28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeAVUk-czHsC"
      },
      "source": [
        "# **4.Train model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXxi_00VzFoa",
        "outputId": "f405451b-fdea-4461-bc1b-9841038e3d39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToyModel(\n",
              "  (nn1): Linear(in_features=784, out_features=120, bias=True)\n",
              "  (nn2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (nn3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 3\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(FP32_model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "FP32_model.to(device) #Put model on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q9E2Mdbza4S"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  #Set the model to train mode\n",
        "  model.train()\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    if use_gpu:\n",
        "      x, y = x.cuda(), y.cuda() #Put data on GPU\n",
        "    optimizer.zero_grad()\n",
        "    #forward\n",
        "    pred = model(x)\n",
        "\n",
        "    #loss\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    #backward\n",
        "    loss.backward()\n",
        "\n",
        "    #optimize\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(x)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  #set model to evaluate mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      if use_gpu:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "      pred = model(x)\n",
        "      test_loss = loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOGK1V5QzkDT",
        "outputId": "c5172543-beb6-445b-d242-d11b30fbc0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.299452  [   32/60000]\n",
            "loss: 1.985761  [ 3232/60000]\n",
            "loss: 1.570050  [ 6432/60000]\n",
            "loss: 0.897553  [ 9632/60000]\n",
            "loss: 1.012562  [12832/60000]\n",
            "loss: 0.643877  [16032/60000]\n",
            "loss: 0.461661  [19232/60000]\n",
            "loss: 0.723728  [22432/60000]\n",
            "loss: 0.521422  [25632/60000]\n",
            "loss: 0.717961  [28832/60000]\n",
            "loss: 1.218322  [32032/60000]\n",
            "loss: 0.777880  [35232/60000]\n",
            "loss: 0.545219  [38432/60000]\n",
            "loss: 0.569304  [41632/60000]\n",
            "loss: 0.559198  [44832/60000]\n",
            "loss: 0.308694  [48032/60000]\n",
            "loss: 0.304523  [51232/60000]\n",
            "loss: 0.473104  [54432/60000]\n",
            "loss: 0.813886  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.0%, Avg loss: 0.000952 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.630806  [   32/60000]\n",
            "loss: 0.527539  [ 3232/60000]\n",
            "loss: 0.535243  [ 6432/60000]\n",
            "loss: 0.725254  [ 9632/60000]\n",
            "loss: 0.941461  [12832/60000]\n",
            "loss: 0.399766  [16032/60000]\n",
            "loss: 0.820962  [19232/60000]\n",
            "loss: 0.413975  [22432/60000]\n",
            "loss: 0.671979  [25632/60000]\n",
            "loss: 0.552520  [28832/60000]\n",
            "loss: 0.414910  [32032/60000]\n",
            "loss: 0.363277  [35232/60000]\n",
            "loss: 0.732840  [38432/60000]\n",
            "loss: 0.396475  [41632/60000]\n",
            "loss: 0.386385  [44832/60000]\n",
            "loss: 0.469653  [48032/60000]\n",
            "loss: 0.258669  [51232/60000]\n",
            "loss: 0.524535  [54432/60000]\n",
            "loss: 0.500962  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.001040 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.508165  [   32/60000]\n",
            "loss: 0.443805  [ 3232/60000]\n",
            "loss: 0.600662  [ 6432/60000]\n",
            "loss: 0.373756  [ 9632/60000]\n",
            "loss: 0.386543  [12832/60000]\n",
            "loss: 0.432205  [16032/60000]\n",
            "loss: 0.453979  [19232/60000]\n",
            "loss: 0.816283  [22432/60000]\n",
            "loss: 0.586312  [25632/60000]\n",
            "loss: 0.501298  [28832/60000]\n",
            "loss: 0.489925  [32032/60000]\n",
            "loss: 0.371134  [35232/60000]\n",
            "loss: 0.292928  [38432/60000]\n",
            "loss: 0.480713  [41632/60000]\n",
            "loss: 0.480322  [44832/60000]\n",
            "loss: 0.499296  [48032/60000]\n",
            "loss: 0.703008  [51232/60000]\n",
            "loss: 0.448041  [54432/60000]\n",
            "loss: 0.588593  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.000961 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(epochs):\n",
        "  print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, FP32_model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxvaRwTC0EYC"
      },
      "source": [
        "# **5.Post Training Quantization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPhhHmRrTjz8"
      },
      "source": [
        "Use Pytorch setup -> use some input data to calibrate -> convert to quantize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8aygP2T0BiJ"
      },
      "outputs": [],
      "source": [
        "#Import quantization\n",
        "from torch.ao.quantization import get_default_qconfig\n",
        "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "from torch.ao.quantization import QConfigMapping\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Fm8gB60M80",
        "outputId": "6391f1d8-3684-4128-dfe8-b8ecb0334838"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToyModel(\n",
              "  (nn1): Linear(in_features=784, out_features=120, bias=True)\n",
              "  (nn2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (nn3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = copy.deepcopy(FP32_model) #copy FP32 model\n",
        "model.eval()\n",
        "model.cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivYilBTvT2Ed"
      },
      "source": [
        "Use Pytorch setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWTvsoF20TPB"
      },
      "outputs": [],
      "source": [
        "#set quantization config\n",
        "qconfig = get_default_qconfig('qnnpack')\n",
        "\n",
        "qconfig_mapping = QConfigMapping().set_global(qconfig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIlhqAhJT5fA"
      },
      "source": [
        "calibrate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVGNBnAF0emP",
        "outputId": "d6f80e3e-406b-4a19-ff24-b126fbec245d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nckusoc/miniconda3/envs/eai-lab/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
            "  torch.has_cuda,\n",
            "/home/nckusoc/miniconda3/envs/eai-lab/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
            "  torch.has_cudnn,\n",
            "/home/nckusoc/miniconda3/envs/eai-lab/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
            "  torch.has_mps,\n",
            "/home/nckusoc/miniconda3/envs/eai-lab/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
            "  torch.has_mkldnn,\n"
          ]
        }
      ],
      "source": [
        "example_inputs = (next(iter(train_loader))[0]) #to know model input data type\n",
        "prepared_model = prepare_fx(model, qconfig_mapping, example_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "023Z64960l1S"
      },
      "outputs": [],
      "source": [
        "def calibrate(model, device, data_loader):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for x, y in data_loader:\n",
        "      x, y = x.to(device), y.to(device) #device\n",
        "      model(x)\n",
        "calibrate(prepared_model, 'cpu', test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Tr42LeT7hv"
      },
      "source": [
        "convert to quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBZbw2V50tpF"
      },
      "outputs": [],
      "source": [
        "PTQ_model = convert_fx(prepared_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEgPtJEK0yGm"
      },
      "source": [
        "check quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1httbLAH0vy_",
        "outputId": "3c2ddf14-1566-454b-8711-b6d79429c779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GraphModule(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.026016514748334885, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.02969088964164257, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.10263293981552124, zero_point=110, qscheme=torch.per_tensor_affine)\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, x):\n",
            "    _input_scale_0 = self._input_scale_0\n",
            "    _input_zero_point_0 = self._input_zero_point_0\n",
            "    quantize_per_tensor = torch.quantize_per_tensor(x, _input_scale_0, _input_zero_point_0, torch.quint8);  x = _input_scale_0 = _input_zero_point_0 = None\n",
            "    view = quantize_per_tensor.view(-1, 784);  quantize_per_tensor = None\n",
            "    nn1 = self.nn1(view);  view = None\n",
            "    nn2 = self.nn2(nn1);  nn1 = None\n",
            "    nn3 = self.nn3(nn2);  nn2 = None\n",
            "    dequantize_4 = nn3.dequantize();  nn3 = None\n",
            "    return dequantize_4\n",
            "    \n",
            "# To see more debug info, please use `graph_module.print_readable()`\n"
          ]
        }
      ],
      "source": [
        "print(PTQ_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSVnY_yG14od"
      },
      "source": [
        "# **6.Quantization Aware Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRYOXSavUHcS"
      },
      "source": [
        "Use Pytorch setup -> use input data to fine-tune model with fake quantize layer -> convert to quantize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZZ-urYH1739"
      },
      "outputs": [],
      "source": [
        "model = copy.deepcopy(FP32_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIeBIDu-UaCP"
      },
      "source": [
        "Use Pytorch setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9WAFSdW194d",
        "outputId": "2bba08b0-4da8-445a-e7db-df3a914c8459"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GraphModule(\n",
              "  (activation_post_process_0): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  (activation_post_process_1): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  (nn1): LinearReLU(\n",
              "    in_features=784, out_features=120, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (activation_post_process_2): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  (nn2): LinearReLU(\n",
              "    in_features=120, out_features=84, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (activation_post_process_3): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  (nn3): Linear(\n",
              "    in_features=84, out_features=10, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (activation_post_process_4): HistogramObserver(min_val=inf, max_val=-inf)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n",
        "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
        "\n",
        "example_inputs = (next(iter(train_loader))[0]) #to know model input data type\n",
        "prepared_model = torch.ao.quantization.quantize_fx.prepare_qat_fx(model, qconfig_mapping, example_inputs) # prepare to quantize model (fuse module (ex:CONV+BN+RELU...)，insert observer)\n",
        "prepared_model.train()\n",
        "prepared_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK7wg5fkUcNj"
      },
      "source": [
        "Training fake quantize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwAYQ2mw2FnL"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 1\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(prepared_model.parameters(), lr=learning_rate, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AipJ-cTa2Jbg",
        "outputId": "4c7d872d-d898-46ba-8ce1-1e111fe68fe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.651754  [   32/60000]\n",
            "loss: 0.480375  [ 3232/60000]\n",
            "loss: 0.444321  [ 6432/60000]\n",
            "loss: 0.174201  [ 9632/60000]\n",
            "loss: 0.322219  [12832/60000]\n",
            "loss: 0.316741  [16032/60000]\n",
            "loss: 0.392883  [19232/60000]\n",
            "loss: 0.299647  [22432/60000]\n",
            "loss: 0.576825  [25632/60000]\n",
            "loss: 0.384341  [28832/60000]\n",
            "loss: 0.266250  [32032/60000]\n",
            "loss: 0.467172  [35232/60000]\n",
            "loss: 0.458649  [38432/60000]\n",
            "loss: 0.337492  [41632/60000]\n",
            "loss: 0.291460  [44832/60000]\n",
            "loss: 0.343571  [48032/60000]\n",
            "loss: 0.143837  [51232/60000]\n",
            "loss: 0.476972  [54432/60000]\n",
            "loss: 0.444388  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.000934 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, prepared_model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, prepared_model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh_DbulY2gCc",
        "outputId": "9235743d-a089-4716-fa40-d217cc4694c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GraphModule(\n",
              "  (activation_post_process_0): HistogramObserver(min_val=-1.0, max_val=1.0)\n",
              "  (activation_post_process_1): HistogramObserver(min_val=-1.0, max_val=1.0)\n",
              "  (nn1): LinearReLU(\n",
              "    in_features=784, out_features=120, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.12837465107440948, max_val=0.11147046834230423)\n",
              "  )\n",
              "  (activation_post_process_2): HistogramObserver(min_val=0.0, max_val=7.486116886138916)\n",
              "  (nn2): LinearReLU(\n",
              "    in_features=120, out_features=84, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.1918354481458664, max_val=0.20926620066165924)\n",
              "  )\n",
              "  (activation_post_process_3): HistogramObserver(min_val=0.0, max_val=8.52192211151123)\n",
              "  (nn3): Linear(\n",
              "    in_features=84, out_features=10, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.304126501083374, max_val=0.36799541115760803)\n",
              "  )\n",
              "  (activation_post_process_4): HistogramObserver(min_val=-11.968474388122559, max_val=16.62274169921875)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prepared_model.cpu()\n",
        "prepared_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufYTZ1S6UgU9"
      },
      "source": [
        "convert to quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD5BwI5Z2ibz"
      },
      "outputs": [],
      "source": [
        "QAT_model = convert_fx(prepared_model) # convert the calibrated model to a quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuVI0ngY2nQx",
        "outputId": "63c2801c-3c61-4604-c8f3-6bed1e706e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GraphModule(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.025902675464749336, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.029976200312376022, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.10237740725278854, zero_point=107, qscheme=torch.per_tensor_affine)\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, x):\n",
            "    _input_scale_0 = self._input_scale_0\n",
            "    _input_zero_point_0 = self._input_zero_point_0\n",
            "    quantize_per_tensor = torch.quantize_per_tensor(x, _input_scale_0, _input_zero_point_0, torch.quint8);  x = _input_scale_0 = _input_zero_point_0 = None\n",
            "    view = quantize_per_tensor.view(-1, 784);  quantize_per_tensor = None\n",
            "    nn1 = self.nn1(view);  view = None\n",
            "    nn2 = self.nn2(nn1);  nn1 = None\n",
            "    nn3 = self.nn3(nn2);  nn2 = None\n",
            "    dequantize_4 = nn3.dequantize();  nn3 = None\n",
            "    return dequantize_4\n",
            "    \n",
            "# To see more debug info, please use `graph_module.print_readable()`\n"
          ]
        }
      ],
      "source": [
        "print(QAT_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zvNOKGs2rxZ"
      },
      "source": [
        "# **7.Compare FP32、PTQ and QAT model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rBQ2eid2py_"
      },
      "outputs": [],
      "source": [
        "def print_size_of_model(model):\n",
        "    \"\"\" Print the size of the model.\n",
        "\n",
        "    Args:\n",
        "        model: model whose size needs to be determined\n",
        "\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size of the model(MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "def compare(model, device, test_loader, quantize=False):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      images, labels = data\n",
        "      images, labels = images.to(device),labels.to(device)\n",
        "      outputs = model(images)\n",
        "      # the class with the highest energy is what we choose as prediction\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  test_loss = 0\n",
        "\n",
        "  print(\"========================================= PERFORMANCE =============================================\")\n",
        "  print_size_of_model(model)\n",
        "  print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format( correct, total,100. * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA0KjHG72xJl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================= PERFORMANCE =============================================\n",
            "Size of the model(MB): 0.423146\n",
            "\n",
            "Accuracy: 8299/10000 (83%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "compare(model=FP32_model, device=\"cpu\", test_loader=test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNksXfbP2z-M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================= PERFORMANCE =============================================\n",
            "Size of the model(MB): 0.110646\n",
            "\n",
            "Accuracy: 8287/10000 (83%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "compare(model=PTQ_model, device=\"cpu\", test_loader=test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7mkJ7-U23TI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================= PERFORMANCE =============================================\n",
            "Size of the model(MB): 0.110646\n",
            "\n",
            "Accuracy: 8464/10000 (85%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "compare(model=QAT_model, device=\"cpu\", test_loader=test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cEwOIA26_w"
      },
      "source": [
        "# **8.Quantize yourself**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L-awfSh-SGs"
      },
      "source": [
        "# 8.1 Quantize layer by layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I8VxP-ksnEl",
        "outputId": "f1e360b9-d4fa-4379-edfe-886da57890a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToyModel(\n",
            "  (nn1): Linear(in_features=784, out_features=120, bias=True)\n",
            "  (nn2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (nn3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = copy.deepcopy(FP32_model).to(\"cpu\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVsOJEukoOyp"
      },
      "outputs": [],
      "source": [
        "class QuantizedLinear(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias):\n",
        "    super(QuantizedLinear, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.scale, self.zero_point = None, None\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.ops.quantized.matmul(x, self.weight.t(), self.scale, self.zero_point).dequantize() + self.bias\n",
        "    output = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(weight.detach().numpy()), np.max(weight.detach().numpy())\n",
        "\n",
        "    s = (max_val - min_val) / (q_max - q_min)\n",
        "    z = round(q_min - min_val / s)\n",
        "    return torch.quantize_per_tensor(weight, s, z, dtype=torch.qint8)\n",
        "\n",
        "  def _calibrate(self, x):\n",
        "    x = x.dequantize()\n",
        "    output = torch.matmul(x, self.weight.t().dequantize())\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(output.detach().numpy()), np.max(output.detach().numpy())\n",
        "    self.scale = (max_val - min_val) / (q_max - q_min)\n",
        "    self.zero_point = round(q_min - min_val / self.scale)\n",
        "\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinear(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'\n",
        "\n",
        "class QuantizedLinearReLU(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias):\n",
        "    super(QuantizedLinearReLU, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.scale, self.zero_point = None, None\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.ops.quantized.matmul(x, self.weight.t(), self.scale, self.zero_point).dequantize() + self.bias\n",
        "    output = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "    output = F.relu(output)\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(weight.detach().numpy()), np.max(weight.detach().numpy())\n",
        "\n",
        "    s = (max_val - min_val) / (q_max - q_min)\n",
        "    z = round(q_min - min_val / s)\n",
        "    return torch.quantize_per_tensor(weight, s, z, dtype=torch.qint8)\n",
        "\n",
        "  def _calibrate(self, x):\n",
        "    x = x.dequantize()\n",
        "    output = F.relu(torch.matmul(x, self.weight.t().dequantize()))\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(output.detach().numpy()), np.max(output.detach().numpy())\n",
        "    self.scale = (max_val - min_val) / (q_max - q_min)\n",
        "    self.zero_point = round(q_min - min_val / self.scale)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinearReLU(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WffWu2lBpqb6"
      },
      "outputs": [],
      "source": [
        "class QuantizedModel(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(QuantizedModel, self).__init__()\n",
        "    self.weight_dic = []\n",
        "    self.bias_dic = []\n",
        "    self.scale, self.zero_point = None, None  #scale and zero point of input layer\n",
        "    self._get_weight()\n",
        "    self.nn1 = QuantizedLinearReLU(in_features=28*28, out_features=120, weight=self.weight_dic[0], bias=self.bias_dic[0])\n",
        "    self.nn2 = QuantizedLinearReLU(in_features=120, out_features=84, weight=self.weight_dic[1], bias=self.bias_dic[1])\n",
        "    self.nn3 = QuantizedLinear(in_features=84, out_features=10, weight=self.weight_dic[2], bias=self.bias_dic[2])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    x = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "    x = self.nn1(x)\n",
        "    x = self.nn2(x)\n",
        "    x = self.nn3(x)\n",
        "    x = x.dequantize()\n",
        "    return x\n",
        "\n",
        "  def _get_weight(self):\n",
        "    for name, paras in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        self.weight_dic.append(paras)\n",
        "      elif \"bias\" in name:\n",
        "        self.bias_dic.append(paras)\n",
        "\n",
        "  def _calibrate(self, input):\n",
        "    self.scale = (np.max(input.detach().numpy()) - np.min(input.detach().numpy())) / 256\n",
        "    self.zero_point = round(np.min(input.detach().numpy())/self.scale)\n",
        "    input = input.view(-1, 28*28)\n",
        "    input = torch.quantize_per_tensor(input, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "\n",
        "    self.nn1._calibrate(input)\n",
        "    input = self.nn1(input)\n",
        "\n",
        "    self.nn2._calibrate(input)\n",
        "    input = self.nn2(input)\n",
        "\n",
        "    self.nn3._calibrate(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yeqe27PcBKJ6",
        "outputId": "eb8d5b8c-f3c4-4f5a-baae-13ccc5862ce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QuantizedModel(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=None, zero_point=None)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=None, zero_point=None)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=None, zero_point=None)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "test_model = QuantizedModel(model)\n",
        "print(test_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LDvPApLA3Ms"
      },
      "source": [
        "這邊需要注意的是，weight的s、z與activation的s、z是分開的，因此每個layer會有兩組(s, z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUEUe7wiqUhh",
        "outputId": "e68e176f-2bf7-4dff-dc1a-6ba7c2164f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QuantizedModel(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.00828688378427543, zero_point=-128)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.007360688845316569, zero_point=-128)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.02116081948373832, zero_point=-12)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Calibrate to compute scale and zero point of model\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  test_model._calibrate(input)\n",
        "  break\n",
        "print(test_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7ByIfDoBuWV"
      },
      "source": [
        "# 8.2 Quantize all layer at the same time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Fr66GAGljY",
        "outputId": "4e258d1e-88ef-4089-aa25-63d25c424ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToyModel(\n",
            "  (nn1): Linear(in_features=784, out_features=120, bias=True)\n",
            "  (nn2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (nn3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = copy.deepcopy(FP32_model).to(\"cpu\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgJZOzAkHnQ0",
        "outputId": "84e90fea-cfd7-4637-e1bf-43f9285f30dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['x', 'view', 'nn1', 'relu', 'nn2', 'relu_1', 'nn3']\n"
          ]
        }
      ],
      "source": [
        "#Check name of all layer\n",
        "train_nodes, eval_nodes = feature_extraction.get_graph_node_names(model)\n",
        "print(train_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLCRuwUlGwKW",
        "outputId": "f9c88f18-9c46-464e-c938-5e42c6ca9cc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00784313725490196, 0.024207195581174363, 0.02985841526704676, 0.0838505464441636]\n",
            "[0, -128, -128, -20]\n"
          ]
        }
      ],
      "source": [
        "scale_dic = []\n",
        "zero_dic = []\n",
        "\n",
        "#Calibrate to compute s、z of all layer at the same time\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  for node in train_nodes:\n",
        "    if node == \"x\" or (\"relu\" in node) or node == \"nn3\":\n",
        "      extractor = feature_extraction.create_feature_extractor(model, [node]).cpu()\n",
        "      output = extractor(input)[node]\n",
        "      q_min, q_max = -128, 127\n",
        "      min_val, max_val = np.min(output.detach().numpy()), np.max(output.detach().numpy())\n",
        "      scale = (max_val - min_val) / (q_max - q_min)\n",
        "      zero = round(q_min - min_val / scale)\n",
        "      scale_dic.append(scale)\n",
        "      zero_dic.append(zero)\n",
        "  break\n",
        "\n",
        "\n",
        "print(scale_dic)\n",
        "print(zero_dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMH0aVyMF2OW"
      },
      "outputs": [],
      "source": [
        "class QuantizedLinear2(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias, scale, zero_point):\n",
        "    super(QuantizedLinear2, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.scale, self.zero_point = scale, zero_point\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.ops.quantized.matmul(x, self.weight.t(), self.scale, self.zero_point).dequantize() + self.bias\n",
        "    output = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(weight.detach().numpy()), np.max(weight.detach().numpy())\n",
        "\n",
        "    s = (max_val - min_val) / (q_max - q_min)\n",
        "    z = round(q_min - min_val / s)\n",
        "    return torch.quantize_per_tensor(weight, s, z, dtype=torch.qint8)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinear(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'\n",
        "\n",
        "class QuantizedLinearReLU2(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias, scale, zero_point):\n",
        "    super(QuantizedLinearReLU2, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.scale, self.zero_point = scale, zero_point\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.ops.quantized.matmul(x, self.weight.t(), self.scale, self.zero_point).dequantize() + self.bias\n",
        "    output = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "    output = F.relu(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(weight.detach().numpy()), np.max(weight.detach().numpy())\n",
        "\n",
        "    s = (max_val - min_val) / (q_max - q_min)\n",
        "    z = round(q_min - min_val / s)\n",
        "    return torch.quantize_per_tensor(weight, s, z, dtype=torch.qint8)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinearReLU(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg4cSFQ-GZak"
      },
      "outputs": [],
      "source": [
        "class QuantizedModel2(nn.Module):\n",
        "  def __init__(self, model, scale, zero_point):\n",
        "    super(QuantizedModel2, self).__init__()\n",
        "    self.weight_dic = []\n",
        "    self.bias_dic = []\n",
        "    self.scale, self.zero_point = scale, zero_point #scale and zero point of input layer\n",
        "\n",
        "    self._get_weight()\n",
        "    self.nn1 = QuantizedLinearReLU2(in_features=28*28, out_features=120, weight=self.weight_dic[0], bias=self.bias_dic[0], scale=self.scale[1], zero_point=self.zero_point[1])\n",
        "    self.nn2 = QuantizedLinearReLU2(in_features=120, out_features=84, weight=self.weight_dic[1], bias=self.bias_dic[1], scale=self.scale[2], zero_point=self.zero_point[2])\n",
        "    self.nn3 = QuantizedLinear2(in_features=84, out_features=10, weight=self.weight_dic[2], bias=self.bias_dic[2], scale=self.scale[3], zero_point=self.zero_point[3])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    x = torch.quantize_per_tensor(x, self.scale[0], self.zero_point[0], dtype=torch.qint8)\n",
        "    x = self.nn1(x)\n",
        "    x = self.nn2(x)\n",
        "    x = self.nn3(x)\n",
        "    x = x.dequantize()\n",
        "    return x\n",
        "\n",
        "  def _get_weight(self):\n",
        "    for name, paras in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        self.weight_dic.append(paras)\n",
        "      elif \"bias\" in name:\n",
        "        self.bias_dic.append(paras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtnJaLCjKAHW",
        "outputId": "d5633ff4-3cd5-4860-9a29-d8f8f3a1df30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QuantizedModel2(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.024207195581174363, zero_point=-128)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.02985841526704676, zero_point=-128)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.0838505464441636, zero_point=-20)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "test_model2 = QuantizedModel2(model, scale=scale_dic, zero_point=zero_dic)\n",
        "\n",
        "print(test_model2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHXqakhEDlns"
      },
      "source": [
        "# 8.3 Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WuAjf-hFdRv",
        "outputId": "0455c3f5-07e2-41b7-bc47-5a4481c25c41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-1.7776, -5.7350, -1.3358, -3.6923, -5.0562,  8.6175, -1.7611,  3.4996,\n",
            "         1.9103,  5.3727], grad_fn=<SelectBackward0>)\n",
            "tensor([-0.2116, -0.6983, -0.3174, -0.5925, -0.6771,  0.4867, -0.3386,  0.6560,\n",
            "         0.7406,  0.9522])\n",
            "tensor([-1.6770, -5.7018, -1.2578, -3.6056, -5.0310,  8.5528, -1.7609,  3.4379,\n",
            "         1.9286,  5.2826])\n"
          ]
        }
      ],
      "source": [
        "FP32_model.to(\"cpu\")\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  output1 = FP32_model(input)[0]    #FP32 model\n",
        "  output2 = test_model(input)[0]    #Quantize layer by layer\n",
        "  output3 = test_model2(input)[0]    #Quantize at the same time\n",
        "  print(output1)\n",
        "  print(output2)\n",
        "  print(output3)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybf4ARuaDsuQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================= PERFORMANCE =============================================\n",
            "\n",
            "Accuracy: 6138/10000 (61%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Quantize layer by layer\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    outputs = test_model(images)\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss = 0\n",
        "\n",
        "print(\"========================================= PERFORMANCE =============================================\")\n",
        "print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format( correct, total,100. * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMA7griAFu7z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================= PERFORMANCE =============================================\n",
            "\n",
            "Accuracy: 8271/10000 (83%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Quantize at the same time\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    outputs = test_model2(images)\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss = 0\n",
        "\n",
        "print(\"========================================= PERFORMANCE =============================================\")\n",
        "print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format( correct, total,100. * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il7XmKXiErg6"
      },
      "source": [
        "# 8.4 Compute MSE of output of each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx-Ij35XoBrx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['x', 'view', 'quantize_per_tensor', 'nn1.matmul', 'nn1.dequantize', 'nn1.nn1_bias', 'nn1.add', 'nn1.quantize_per_tensor', 'nn1.relu', 'nn2.matmul', 'nn2.dequantize', 'nn2.nn2_bias', 'nn2.add', 'nn2.quantize_per_tensor', 'nn2.relu', 'nn3.matmul', 'nn3.dequantize', 'nn3.nn3_bias', 'nn3.add', 'nn3.quantize_per_tensor', 'dequantize']\n",
            "['x', 'view', 'quantize_per_tensor', 'nn1.matmul', 'nn1.dequantize', 'nn1.nn1_bias', 'nn1.add', 'nn1.quantize_per_tensor', 'nn1.relu', 'nn2.matmul', 'nn2.dequantize', 'nn2.nn2_bias', 'nn2.add', 'nn2.quantize_per_tensor', 'nn2.relu', 'nn3.matmul', 'nn3.dequantize', 'nn3.nn3_bias', 'nn3.add', 'nn3.quantize_per_tensor', 'dequantize']\n"
          ]
        }
      ],
      "source": [
        "train_nodes, eval_nodes = feature_extraction.get_graph_node_names(test_model)\n",
        "print(train_nodes)\n",
        "train_nodes, eval_nodes = feature_extraction.get_graph_node_names(test_model2)\n",
        "print(train_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plwghz87n5DL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE of layer quantize_per_tensor is 0.582194447517395\n",
            "MSE of layer nn1.relu is 1.2625266313552856\n",
            "MSE of layer nn2.relu is 1.5307530164718628\n",
            "MSE of layer dequantize is 11.38371467590332\n"
          ]
        }
      ],
      "source": [
        "#Compare MSE of 2 different methods\n",
        "\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  for train_node in train_nodes:\n",
        "    if train_node == \"quantize_per_tensor\" or \"relu\" in train_node or train_node == \"dequantize\":\n",
        "      extractor1 = feature_extraction.create_feature_extractor(test_model, [train_node]).cpu()\n",
        "      output1 = extractor1(input)[train_node]\n",
        "      extractor2 = feature_extraction.create_feature_extractor(test_model2, [train_node]).cpu()\n",
        "      output2 = extractor2(input)[train_node]\n",
        "      mse = F.mse_loss(output1.dequantize(), output2.dequantize())\n",
        "      print(f'MSE of layer {train_node} is {mse}')\n",
        "  break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
