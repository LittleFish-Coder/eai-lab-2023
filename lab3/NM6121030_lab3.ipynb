{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWptM2AFfExI"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4xXb5NnfGdd"
      },
      "outputs": [],
      "source": [
        "#change to your datapath\n",
        "# %cd /content/drive/MyDrive/EAI_2023_fall/StudentID_lab3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uTY8C-LBhDTX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from resnet18 import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "io7k6zHGb1OJ"
      },
      "outputs": [],
      "source": [
        "# Create SummaryWriter\n",
        "writer = SummaryWriter(\"./tensorboard\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oqc_D_uEhc1O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "#檢查是否可用gpu\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CrsS13SChs9b"
      },
      "outputs": [],
      "source": [
        "# setting parameter\n",
        "EPOCH = 40\n",
        "pre_epoch = 0\n",
        "BATCH_SIZE = 128\n",
        "lr = 0.05\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3LzusjVfhxJ3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "[0.4917974  0.48258492 0.44749185] [0.24606577 0.24262413 0.2611737 ]\n",
            "[0.4917974  0.48258492 0.44749185] [0.24606577 0.24262413 0.2611737 ]\n"
          ]
        }
      ],
      "source": [
        "# 計算normalization需要的mean & std\n",
        "def get_mean_std(dataset, ratio=0.3):\n",
        "    # Get mean and std by sample ratio\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=int(len(dataset)*ratio), shuffle=True, num_workers=2)\n",
        "\n",
        "    data = next(iter(dataloader))[0]     # get the first iteration data\n",
        "    mean = np.mean(data.numpy(), axis=(0,2,3))\n",
        "    std = np.std(data.numpy(), axis=(0,2,3))\n",
        "    return mean, std\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_mean, train_std = get_mean_std(train_dataset)\n",
        "test_mean, test_std = train_mean, train_std # in usual case, we don't have test dataset, we use train mean & std as test mean & std\n",
        "print(train_mean, train_std)\n",
        "print(test_mean, test_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5NGnL4rP6ofO"
      },
      "outputs": [],
      "source": [
        "# data augmentation & normalization\n",
        "transform_train = transforms.Compose([\n",
        "    # data augmentation\n",
        "    transforms.ToTensor(),\n",
        "    # data normalization    # standardization: (image - train_mean) / train_std\n",
        "    transforms.Normalize(train_mean, train_std)\n",
        "])\n",
        "\n",
        "# for test, we only need to normalize data\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # data normalization    # standardization: (image - train_mean) / train_std\n",
        "    transforms.Normalize(train_mean, train_std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaJyYUXj6rNW"
      },
      "outputs": [],
      "source": [
        "# dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# 檢查training dataset長怎麼樣\n",
        "#print(\"trainset length: \", len(trainset))\n",
        "#print(\"classes: \", trainset.classes)\n",
        "#image, label = trainset[0]\n",
        "#print(\"image shape: \", image.shape)\n",
        "#print(\"label: \", label)\n",
        "\n",
        "# Cifar-10的標籤: ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# split validation dataset\n",
        "torch.manual_seed(43)     # 確保每次獲得相同的驗證集\n",
        "val_size = 5000       # 取5000張驗證集(0.1 of trainset)\n",
        "train_size = len(trainset) - val_size\n",
        "train_ds, val_ds = random_split(trainset, [train_size, val_size])\n",
        "print(\"train length: \", len(train_ds))\n",
        "print(\"val length: \", len(val_ds))\n",
        "print(\"test length: \", len(test_ds))\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)   #生成batch\n",
        "valloader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgJejgUOVviV"
      },
      "outputs": [],
      "source": [
        "# learning rate shedule\n",
        "def adjust_learning_rate(optim, epoch):\n",
        "    # define your lr scheduler\n",
        "\n",
        "\n",
        "\n",
        "    for param_group in optim.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfBB9RKK6t8e"
      },
      "outputs": [],
      "source": [
        "#是否pretrain\n",
        "net = ResNet(ResBlock).to(device)\n",
        "\n",
        "# 定義損失函數和優化方式\n",
        "criterion = nn.CrossEntropyLoss()  #loss function\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4) #優化方式為mini-batch momentum-SGD，採用L2正則化（權重衰減）\n",
        "schecdular = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.5,last_epoch=-1)\n",
        "# train\n",
        "if __name__ == \"__main__\":\n",
        "    train_loss, train_acc = [], []\n",
        "    val_loss, val_acc = [], []\n",
        "    for epoch in range(pre_epoch, EPOCH):\n",
        "        net.train()\n",
        "        sum1_loss, sum2_loss = 0.0, 0.0\n",
        "        correct = 0.0\n",
        "        total = 0.0\n",
        "        print('\\nEpoch: %d' % (epoch + 1))\n",
        "        for i, traindata in enumerate(trainloader, 0):\n",
        "            # prepare data\n",
        "            length = len(trainloader)\n",
        "            inputs, train_labels = traindata\n",
        "            inputs, train_labels = inputs.to(device), train_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward\n",
        "            train_outputs = net(inputs)\n",
        "            trainloss = criterion(train_outputs, train_labels)\n",
        "            trainloss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 每訓練1個batch的loss和acc\n",
        "            sum1_loss += trainloss.item()\n",
        "            _, predicted = torch.max(train_outputs.data, 1)     # 取得分數最高的那個類 (outputs.data的index), train_outputs.data.shape is torch.Size([128, 10])\n",
        "            # print(torch.max(train_outputs.data, 1)[1])\n",
        "            total += train_labels.size(0)\n",
        "            correct += predicted.eq(train_labels.data).cpu().sum()\n",
        "\n",
        "\n",
        "        schecdular.step()\n",
        "        writer.add_scalar(\"lr\", optimizer.param_groups[0]['lr'], epoch)\n",
        "        # adjust_learning_rate(optimizer, epoch)\n",
        "        print(\"learning rate: \",  optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        loss1 = sum1_loss / (i + 1) # i is equal to number of batches\n",
        "        acc1 = correct / total\n",
        "        print(\"Train loss: %.3f | Train acc: %.3f\" % (loss1, acc1))\n",
        "        train_loss.append(loss1)\n",
        "        train_acc.append(acc1.item())\n",
        "\n",
        "        # 每個epoch完用val驗證\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for j, valdata in enumerate(valloader, 0):\n",
        "                net.eval()\n",
        "                images, val_labels = valdata\n",
        "                images, val_labels = images.to(device), val_labels.to(device)\n",
        "                val_outputs = net(images)\n",
        "                valloss = criterion(val_outputs, val_labels)\n",
        "\n",
        "                sum2_loss += valloss.item()\n",
        "                _, predicted = torch.max(val_outputs.data, 1)\n",
        "                total += val_labels.size(0)\n",
        "                correct += (predicted == val_labels).sum()\n",
        "\n",
        "            loss2 = sum2_loss / (j + 1)\n",
        "            acc2 = correct / total\n",
        "            print(\"Val loss: %.3f | Val acc: %.3f\" % (loss2, acc2))\n",
        "            val_loss.append(loss2)\n",
        "            val_acc.append(acc2.item())\n",
        "\n",
        "    # 用test測試\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        sum3_loss = 0.0\n",
        "        for k, testdata in enumerate(testloader, 0):\n",
        "            net.eval()\n",
        "            imgs, test_labels = testdata\n",
        "            imgs, test_labels = imgs.to(device), test_labels.to(device)\n",
        "            test_outputs = net(imgs)\n",
        "            testloss = criterion(test_outputs, test_labels)\n",
        "\n",
        "            sum3_loss += testloss.item()\n",
        "            _, predicted = torch.max(test_outputs.data, 1)\n",
        "            total += test_labels.size(0)\n",
        "            correct += (predicted == test_labels).sum()\n",
        "\n",
        "        loss3 = sum3_loss / (k + 1)\n",
        "        acc3 = correct / total\n",
        "        print(\"Test loss: %.3f | Test acc: %.3f\" % (loss3, acc3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-QShVIgmicX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plot accuracy curve\n",
        "plt.figure(0)\n",
        "plt.plot(range(1,EPOCH+1,1), np.array(train_acc), 'r-', label= \"train accuracy\")\n",
        "plt.plot(range(1,EPOCH+1,1), np.array(val_acc), 'b-', label= \"val accuracy\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "#plt.ylim(0.6, 1.1)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot loss curve\n",
        "plt.figure(1)\n",
        "plt.plot(range(1,EPOCH+1,1), np.array(train_loss), 'r-', label= \"train loss\")\n",
        "plt.plot(range(1,EPOCH+1,1), np.array(val_loss), 'b-', label= \"val loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
